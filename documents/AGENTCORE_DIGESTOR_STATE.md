# AGENTCORE_DIGESTOR_STATE.md

## 1. Overview

**AgentCore Digestor** √® un agente di ingestion dati costruito su **AWS Bedrock AgentCore** con architettura **tool-driven**, progettato per:

- ingerire file forniti dall‚Äôutente via S3
- archiviare sempre il raw originale
- normalizzare e validare i dati
- caricare dati tabulari puliti in **Iceberg (Athena/Glue)**
- gestire formati multipli in modo controllato e dichiarativo

Il progetto √® **AWS-only**, non usa Snowflake o Databricks, ed √® orientato a:
- robustezza
- tracciabilit√†
- ripetibilit√†
- assenza di ‚Äúmagia implicita‚Äù

---

## 2. Principi Architetturali Fondamentali

Questi principi sono **non negoziabili**:

1. **Tool-driven only**
   - L‚Äôagente non processa mai direttamente i file
   - Ogni azione √® delegata a un tool o a una Lambda

2. **Raw first**
   - Ogni richiesta di ingestion salva SEMPRE una copia RAW del file originale

3. **Single Source of Truth**
   - Dopo la normalizzazione, **solo il file normalizzato pu√≤ essere ingerito**
   - N√© il file originale n√© quello convertito sono mai caricati in Iceberg

4. **Pipeline deterministica**
   - L‚Äôingestion segue sempre la stessa sequenza
   - Nessun salto di step, nessuna scorciatoia

5. **Fail fast, explain clearly**
   - Se qualcosa non √® supportato, l‚Äôagente:
     - lo spiega
     - non forza il caricamento
     - non inventa soluzioni

---

## 3. Bucket S3 e Ruoli

### Upload (input utente)

agentcore-digestor-upload-raw-dev

- File caricati manualmente dall‚Äôutente
- Usato come input iniziale
- Contiene anche:
  - `converted/`
  - `normalized/`

---

### Archive (RAW storico)

agentcore-digestor-archive-dev

Struttura:

<extension>/<YYYY-MM-DD>/<filename>


- Contiene **sempre** il file originale
- Serve per audit, rollback, riprocessamento

---

### Iceberg Warehouse

agentcore-digestor-iceberg-bronze-dev


Struttura:

warehouse/<table_name>/data/


- Contiene SOLO Parquet puliti
- Scrittura tramite Lambda `load_into_iceberg`
- Mai file sporchi o raw

---

## 4. Pipeline Canonica di Ingestion

Questa √® la **pipeline ufficiale**.  
Qualsiasi nuova feature deve rispettarla.

detect_file_type
‚Üí raw_ingest
‚Üí convert_semi_tabular (se necessario)
‚Üí analyze_schema
‚Üí validate_data
‚Üí schema_normalizer
‚Üí load_into_iceberg (normalized_path ONLY)
‚Üí create_iceberg_table


---

## 5. Tool: Stato e Responsabilit√†

### detect_file_type
- Determina:
  - formato (csv, tsv, txt, xlsx, json, ndjson, pdf‚Ä¶)
  - classe (tabular / semi-tabular / non-tabular)
- NON legge i dati
- NON converte

---

### raw_ingest
- Copia **sempre** il file originale nell‚Äôarchive bucket
- Non interpreta il contenuto
- √à obbligatorio per ogni ingestion

---

### convert_semi_tabular
Converte formati non direttamente ingestibili:

| Formato | Output |
|-------|-------|
| JSON array | NDJSON |
| XLSX/XLS | CSV |
| TXT | CSV (delimiter autodetect) |
| CSV/TSV | passthrough |

Scrive in:

agentcore-digestor-upload-raw-dev/converted/


---

### analyze_schema
- Analizza **solo file tabulari**
- Richiede sempre:
  - `file_s3_path`
  - `file_format`
- Inferisce schema grezzo

---

### validate_data
- Controlla coerenza row-level
- Non rimuove righe
- Serve solo come segnale diagnostico

---

### schema_normalizer  **(CRITICO)**
- Inferisce tipo finale con majority rule
- Converte valori
- **Rimuove righe invalide**
- Scrive un CSV pulito in:

agentcore-digestor-upload-raw-dev/normalized/<filename>_normalized.csv


Restituisce:
- `normalized_path`
- `schema_normalized`
- numero righe rimosse

üëâ **Il normalized file √® la SINGLE SOURCE OF TRUTH**

---

### load_into_iceberg
- Tool ‚Üí Lambda dockerizzata
- Legge SOLO il file normalizzato
- Scrive Parquet nel warehouse Iceberg
- Non filtra dati (si fida del normalizer)

---

### create_iceberg_table
- Crea la tabella Iceberg se non esiste
- Usa schema normalizzato
- Non carica dati

---

## 6. Stato Attuale per Formato

| Formato | Stato |
|------|------|
| CSV | ‚úÖ completo |
| TXT | ‚úÖ completo |
| XLSX/XLS | ‚úÖ completo |
| JSON array | ‚ö†Ô∏è policy non definitiva |
| NDJSON | ‚ö†Ô∏è analisi OK, ingestion non definitiva |
| PDF/DOC | ‚úÖ solo RAW + descrizione |

---

## 7. JSON: Stato e Scelte Aperte

Attuale:
- JSON array ‚Üí convertibile a NDJSON
- Analisi schema funziona
- Ingestion **non ancora garantita**

Decisioni future possibili:
- Convertire JSON ‚Üí CSV ‚Üí pipeline standard
- Supportare NDJSON direttamente in Iceberg
- Limitare JSON a sola analisi + archiviazione

‚ö†Ô∏è **Serve una policy definitiva prima del deploy**

---

## 8. System Prompt: Regole Chiave

Il system prompt impone:
- uso obbligatorio dei tool
- ordine rigido della pipeline
- uso esclusivo del `normalized_path` per ingestion
- rifiuto esplicito di formati non supportati

Il prompt √® considerato **parte dell‚Äôarchitettura**, non testo decorativo.

---

## 9. Git Workflow (Obbligatorio)

- Ogni feature ‚Üí nuovo branch:

feature/<descrizione>

- Commit frequenti nei punti stabili
- Test manuali via `agentcore invoke --dev`
- Merge in `main` **solo quando stabile**
- Branch eliminato dopo merge

---

## 10. Roadmap Prossimi Step

### A. Excel edge cases
- sheet non specificato
- colonne vuote
- date miste

### B. JSON policy definitiva
- decidere ingestion s√¨/no
- definire conversione canonica

### C. Configurabilit√†
- mode = drop_invalid / keep_nulls
- soglia majority rule
- naming table

### D. Ingestion log / metadata table
- tabella Athena non-Iceberg
- traccia file, path, timestamp, esito

### E. Hardening pre-deploy
- error handling
- idempotenza
- documentazione finale

---

## 11. Stato Finale

Il progetto √®:
- **strutturalmente solido**
- **concettualmente coerente**
- **pronto per decisioni finali su JSON e deploy**

Questo documento rappresenta **la fonte ufficiale di verit√†** per riprendere il lavoro in una nuova chat o contesto.